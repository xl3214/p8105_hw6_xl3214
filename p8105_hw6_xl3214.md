p8105_hw6_xl3214
================
Xuan Lu
2023-11-28

## Problem 1: The Homocide Dataset by Washington Post

### Step 1: Data Import and Preparation for Analysis

``` r
q1_raw <- read.csv(file = "https://raw.githubusercontent.com/washingtonpost/data-homicides/master/homicide-data.csv", 
                   na = c("", "NA", "Unknown"))
```

The raw data has 12 variables and 52179 observations. Variables include:
*uid, reported_date, victim_last, victim_first, victim_race, victim_age,
victim_sex, city, state, lat, lon, disposition*.

``` r
q1_for_analysis <- q1_raw |>
  # Create city_state variable in the format of "City, State"
  mutate(city_state = paste(city, state, sep = ", "), 
         # victim_age needs to be numeric
         victim_age = as.numeric(victim_age), 
         # Create binary variable indicating whether the homicide is solved
         resolved = factor(ifelse(disposition == "Closed by arrest", 1, 0),
                           levels = c(0, 1),
                           labels = c("No", "Yes"),
                           ordered = TRUE)) |>
  # Omit cities Dallas, TX; Phoenix, AZ; and Kansas City, MO – these don’t report victim race
  # Omit Tulsa, AL – this is a data entry mistake
  filter(!(city_state %in% c("Dallas, TX", "Phoenix, AZ", "Kansas City, MO", "Tulsa, AL"))) |>
  # Limit analysis to victim_race is white or black
  filter(victim_race %in% c("White", "Black")) |>
  select(city_state, resolved, victim_age, victim_sex, victim_race)
```

After data cleaning and validation, which excludes cities Dallas, TX;
Phoenix, AZ; Kansas City, MO; and Tulsa, AL, and limiting the
victim_race to only black or white, the dataset pertained to analysis
stage includes 5 variables and 39693 observations.

### Step 2: Fit Logistic Regression for Baltimore, MD

Fit a logistic regression model for the city of Baltimore, MD using
`glm()`, with the binary variable indicating whether the homicide is
solved as the outcome, and victim age, sex, and race as predictors. Use
`broom::tidy()` to extract the model estimates and confidence intervals,
focusing on the adjusted odds ratio for male vs. female victims.

``` r
model_baltimore <- q1_for_analysis |>
  # Filter the dataset to include only rows where city_state is "Baltimore, MD"
  filter(city_state == "Baltimore, MD") |>
  # Fit a logistic regression model with resolved as the response variable and victim_age, victim_sex, and victim_race as predictors
  glm(resolved ~ victim_age + victim_sex + victim_race, 
                       data = _, family = "binomial") |>
  # Use broom::tidy to convert the glm model object into a tidy dataframe and calculate confidence intervals
  broom::tidy(conf.int = TRUE) |>
  # Filter the results to keep only the row for the term "victim_sexMale"
  filter(term == "victim_sexMale") |>
  # Calculate the odds ratio and its confidence intervals by exponentiating the estimates
  mutate(OR = exp(estimate),
         OR_lower_ci = exp(conf.low),
         OR_upper_ci = exp(conf.high),
         p_value = p.value) |>
  # Select only the relevant columns for the final output
  select(term, OR, OR_lower_ci, OR_upper_ci, p_value)
```

The odds of the homicide being solved for male victims is estimated to
be 0.4255117 times that of female victims, adjusting for victim age and
race. A reasonable (alpha = 0.05) range of estimates for the true OR is
between 0.3241908 and 0.5575508. The p-value is 6.2551188^{-10},
indicating a statistically significant association between victim’s sex
and the resolution of the homicide.

### Step 3: Logistic Regression for Each City and Dataframe with Estimated ORs and CIs

Apply `glm()` to each city in the dataset to get the adjusted odds ratio
for male vs. female victims using `tidyverse::nest()`, `purrr::map()`,
and `tidyverse::unnest()`.

``` r
model_all_cities <- q1_for_analysis |>
  # the data is being grouped by city_state and stored in "data"
  nest(data = -city_state) |>
  # fit the glm model for each nested dataframe
  mutate(models = map(data, ~glm(resolved ~ victim_age + victim_sex + victim_race, 
                                 family = "binomial", data = .))) |>
  # tidy the glm models and calculate ORs and CIs
  mutate(tidy_models = map(models, ~broom::tidy(.x, conf.int = TRUE))) |>
  # extract only the rows corresponding to the term "victim_sexMale"
  mutate(tidy_models = map(tidy_models, ~filter(.x, term == "victim_sexMale"))) |>
  # calculate the odds ratios and confidence intervals
  mutate(tidy_models = map(tidy_models, ~mutate(.x, 
                                                OR = exp(estimate),
                                                OR_lower_ci = exp(conf.low),
                                                OR_upper_ci = exp(conf.high), 
                                                p_value = p.value))) |>
  # remove the columns that are no longer needed
  select(-data, -models) |>
  # unnest the tidy_models to get a flat dataframe
  unnest(cols = tidy_models) |>
  # select the final columns of interest
  select(city_state, term, OR, OR_lower_ci, OR_upper_ci, p_value)

model_all_cities |>
  arrange(desc(OR)) |> 
  slice(1:5) |>
  knitr::kable(digits = 3)
```

| city_state      | term           |    OR | OR_lower_ci | OR_upper_ci | p_value |
|:----------------|:---------------|------:|------------:|------------:|--------:|
| Albuquerque, NM | victim_sexMale | 1.767 |       0.825 |       3.762 |   0.139 |
| Stockton, CA    | victim_sexMale | 1.352 |       0.626 |       2.994 |   0.447 |
| Fresno, CA      | victim_sexMale | 1.335 |       0.567 |       3.048 |   0.496 |
| Nashville, TN   | victim_sexMale | 1.034 |       0.681 |       1.556 |   0.873 |
| Richmond, VA    | victim_sexMale | 1.006 |       0.483 |       1.994 |   0.987 |

Based on the output table, Albuquerque, NM has the highest odds ratio
when comparing victims whose sex is male to those whose sex is female.
With a p-value of 0.139, however, we do not have sufficient evidence to
conclude that the odds of resolving the homicide is statistically higher
among males than females in Albuquerque, NM, after adjusting for victim
age and race.

### Step 4: Plotting the Results

``` r
# Start with the model_all_cities dataset
model_all_cities |> 
  # Initialize a ggplot with city_state on the x-axis and OR on the y-axis. Cities are reordered by OR for a meaningful display.
  ggplot(aes(x = fct_reorder(city_state, OR), y = OR)) +
  # Add points to the plot for each city's OR value.
  geom_point() +
  # Add vertical error bars for each point to represent the confidence interval of the OR, with a specified width.
  geom_errorbar(aes(ymin = OR_lower_ci, ymax = OR_upper_ci), width = 0.2) +
  # Flip the coordinates to have cities on the y-axis and OR on the x-axis for easier reading of long city names.
  coord_flip() +
  # Add labels to the x and y axes, and a title to the plot.
  labs(x = "City", y = "Odds Ratio (Male vs Female Victims, reference = Female)",
       title = "Adjusted Odds Ratios for Solving Homicides by City")
```

![](p8105_hw6_xl3214_files/figure-gfm/Q1-plotting%20of%20ORs%20for%20all%20cities-1.png)<!-- -->

## Problem 2: NYC Central Park Weather

``` r
weather_df = 
  rnoaa::meteo_pull_monitors(
    c("USW00094728"),
    var = c("PRCP", "TMIN", "TMAX"), 
    date_min = "2022-01-01",
    date_max = "2022-12-31") |>
  mutate(
    name = recode(id, USW00094728 = "CentralPark_NY"),
    tmin = tmin / 10,
    tmax = tmax / 10) |>
  select(name, id, everything())
```

    ## using cached file: /Users/kristal99/Library/Caches/org.R-project.R/R/rnoaa/noaa_ghcnd/USW00094728.dly

    ## date created (size, mb): 2023-11-28 16:40:03.620223 (8.544)

    ## file min/max dates: 1869-01-01 / 2023-11-30

### Steps:

- **Bootstrap Sampling:** Generate 5000 bootstrap samples of the
  dataset.
- **Fit Linear Regression:** For each sample, fit a linear regression
  model with *tmax* as the response variable and *tmin* and *prcp* as
  the predictors.
- **Extract R-square:** Use `broom::glance()` to extract the R-square
  value from each fitted model.
- **Calculate log(β_tmin x β_prcp):** Use `broom::tidy()` to get the
  regression coefficients from each model, then calculate the log
  product of the *tmin* and *prcp* coefficients.
- **Plot Distributions:** Plot the distributions of the 5000 R-square
  values and the 5000 log(β_hat_1 \* β_hat_2) values.
- **Calculate Confidence Intervals:** Determine the 2.5th and 97.5th
  percentiles of the bootstrap estimates to find the 95% confidence
  intervals for both statistics.

``` r
bootstrap_sample <- function(df) {
  sample_indices <- sample(seq_len(nrow(df)), size = nrow(df), replace = TRUE)
  sample_df <- df[sample_indices, ]
  
  fit <- lm(tmax ~ tmin + prcp, data = sample_df)
  glance_stats <- glance(fit)
  tidy_stats <- tidy(fit)
  
  # Extract R-squared
  r_squared <- glance_stats$r.squared
  
  # Compute the log product of tmin and prcp coefficients
  # Make sure to handle cases where the product might be non-positive
  coef_tmin <- tidy_stats$estimate[tidy_stats$term == "tmin"]
  coef_prcp <- tidy_stats$estimate[tidy_stats$term == "prcp"]
  log_product <- ifelse(coef_tmin * coef_prcp > 0, log(coef_tmin * coef_prcp), NA)
  
  return(list(r_squared = r_squared, log_product = log_product))
}

# Perform the bootstrap sampling and compute statistics for each sample
bootstrap_results <- replicate(5000, bootstrap_sample(weather_df), simplify = FALSE)

# Convert the list of bootstrap results into a data frame
bootstrap_results_df <- do.call(rbind.data.frame, bootstrap_results)
```

``` r
set.seed(123)  # For reproducibility of bootstrap samples

# Define the bootstrap function
bootstrap_sample <- function(df) {
  sample_indices <- sample(seq_len(nrow(df)), size = nrow(df), replace = TRUE)
  sample_df <- df[sample_indices, ]
  
  fit <- lm(tmax ~ tmin + prcp, data = sample_df)
  glance_stats <- glance(fit)
  tidy_stats <- tidy(fit)
  
  # Extract R-squared
  r_squared <- glance_stats$r.squared
  
  # Compute the log product of tmin and prcp coefficients
  # Make sure to handle cases where the product might be non-positive
  coef_tmin <- tidy_stats$estimate[tidy_stats$term == "tmin"]
  coef_prcp <- tidy_stats$estimate[tidy_stats$term == "prcp"]
  log_product <- ifelse(coef_tmin * coef_prcp > 0, log(coef_tmin * coef_prcp), NA)
  
  return(list(r_squared = r_squared, log_product = log_product))
}

# Generate bootstrap samples and compute statistics for each
bootstrap_results <- replicate(5000, bootstrap_sample(weather_df), simplify = FALSE) 
# Convert the list of bootstrap results into a data frame
bootstrap_results_df <- do.call(rbind.data.frame, bootstrap_results)

# Histogram of R^2
hist(bootstrap_results_df[, "r_squared"], main = "Histogram of R^2", xlab = "R^2", breaks = 20, col = 'blue')
```

![](p8105_hw6_xl3214_files/figure-gfm/Q2-bootstrap%20sampling%20and%20plot-1.png)<!-- -->

``` r
# QQ-plot of R^2
qqnorm(bootstrap_results_df[, "r_squared"], main = "QQ-Plot of R^2")
qqline(bootstrap_results_df[, "r_squared"], col = 'red')
```

![](p8105_hw6_xl3214_files/figure-gfm/Q2-bootstrap%20sampling%20and%20plot-2.png)<!-- -->

``` r
# Histogram of log(beta_tmin * beta_prcp)
hist(bootstrap_results_df[, "log_product"], main = "Histogram of log(beta_tmin * beta_prcp)", xlab = "log(beta_tmin * beta_prcp)", breaks = 20, col = 'blue')
```

![](p8105_hw6_xl3214_files/figure-gfm/Q2-bootstrap%20sampling%20and%20plot-3.png)<!-- -->

``` r
# QQ-plot of log(beta_tmin * beta_prcp)
qqnorm(bootstrap_results_df[, "log_product"], main = "QQ-Plot of log(beta_tmin * beta_prcp)")
qqline(bootstrap_results_df[, "log_product"], col = 'red')
```

![](p8105_hw6_xl3214_files/figure-gfm/Q2-bootstrap%20sampling%20and%20plot-4.png)<!-- -->

### R-square

Based on the histogram, the distribution of R-square appears to be
unimodal and symmetric, centered around 0.90. The shape of the histogram
suggests that most bootstrap samples resulted in R-square values close
to this central value, indicating consistent performance of the
regression model across bootstrap samples. Based on the qq-plot, The
slight deviation at the upper tail (top-right part of the plot) might
indicate a minor departure from normality, with the bootstrap R-square
values being slightly higher than what would be expected under a perfect
normal distribution.

### log(β_tmin x β_prcp)

Based on the histogram, the distribution is unimodel and slightly skewed
to the left, indicating that the log-transformed product of the
coefficients tends to have an inconsistent estimate across bootstrap
samples.qq-plot shows apparent deviation from the line at both tails,
particularly the lower tail, suggests that the log-transformed product
might have a distribution with heavier tails than the normal
distribution, indicating some outliers or extreme values.

``` r
# Calculating the 95% confidence intervals for R^2
r_squared_ci <- quantile(bootstrap_results_df[, "r_squared"], 
                         probs = c(0.025, 0.975), 
                         na.rm = TRUE)

# Calculating the 95% confidence intervals for log(beta_tmin * beta_prcp)
log_product_ci <- quantile(bootstrap_results_df[, "log_product"], 
                           probs = c(0.025, 0.975), 
                           na.rm = TRUE)

# Output the confidence intervals
print("95% Confidence Interval for R-square")
```

    ## [1] "95% Confidence Interval for R-square"

``` r
r_squared_ci |> knitr::kable()
```

|       |         x |
|:------|----------:|
| 2.5%  | 0.8882079 |
| 97.5% | 0.9402552 |

``` r
print("95% Confidence Interval for log(β_tmin x β_prcp)")
```

    ## [1] "95% Confidence Interval for log(β_tmin x β_prcp)"

``` r
log_product_ci |> knitr::kable()
```

|       |         x |
|:------|----------:|
| 2.5%  | -9.063214 |
| 97.5% | -4.619267 |
